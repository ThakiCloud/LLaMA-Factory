### model
model_name_or_path: zai-org/GLM-4.5-Air
trust_remote_code: true

### method
stage: sft
do_train: true
finetuning_type: lora
lora_rank: 8
lora_target: all  #  v_proj,k_proj,up_proj,q_proj,o_proj,gate_proj,down_proj

### dataset
dataset: sampled-50k
template: glm4
cutoff_len: 2048
# default_system: "You are a helpful AI assistant."  # System prompt 자동 추가
# max_samples: 50000  # 전체 50K 데이터 사용
overwrite_cache: true
preprocessing_num_workers: 16
dataloader_num_workers: 4

### output
output_dir: saves/glm4.5-air/lora/sft
logging_steps: 10
save_steps: 100  
# save_total_limit: 2
plot_loss: true
overwrite_output_dir: true
save_only_model: false
report_to: wandb

### train
per_device_train_batch_size: 4
gradient_accumulation_steps: 4 # 결과 체크
learning_rate: 1.0e-4 2e-5
num_train_epochs: 1.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
gradient_checkpointing: true  # 메모리 절약 (필수!)
ddp_timeout: 180000000
resume_from_checkpoint: null

### deepspeed
deepspeed: examples/deepspeed/ds_z3_config.json  

### eval
# eval_dataset: ko-lima-vicuna
# val_size: 0.1
# per_device_eval_batch_size: 1
# eval_strategy: steps
# eval_steps: 500


